# ============================================================
# ğŸ•‹ Hajj Data Integration & Fuzzy Matching System
# ============================================================
# Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ¯Ù…Ø¬ Ù…Ù„ÙØ§Øª ExcelØŒ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ØŒ ÙˆØªÙ†ÙÙŠØ° Ù…Ù‚Ø§Ø±Ù†Ø© Fuzzy Matching
# ============================================================

# ==============================
# ğŸ“¦ Import Libraries
# ==============================
from google.colab import files, data_table
import pandas as pd
import numpy as np
import time
import warnings
warnings.filterwarnings('ignore')

# ============================================================
# ğŸ”¹ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø¯Ù…Ø¬ Ù…Ù„ÙØ§Øª Ø´Ø±ÙƒØ§Øª Ø§Ù„Ø­Ø¬
# ============================================================

# 1ï¸âƒ£ Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ø§Ù„Ø¬Ù‡Ø§Ø²
uploaded = files.upload()
files_list = list(uploaded.keys())

# 2ï¸âƒ£ Ù‚Ø±Ø§Ø¡Ø© ÙƒÙ„ Ù…Ù„Ù ÙˆØ¯Ù…Ø¬Ù‡ ÙÙŠ DataFrame ÙˆØ§Ø­Ø¯
dfs = [pd.read_excel(f) for f in files_list]
df_final = pd.concat(dfs, ignore_index=True)
df_final.columns = df_final.columns.str.strip()

# 3ï¸âƒ£ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù‡Ù…Ø© Ù„Ù„Ø£Ø³Ù…Ø§Ø¡
arabic_cols = ['Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ© Ø¹Ø±Ø¨ÙŠ', 'Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ©', 'Ø´Ø±ÙƒØ© Ø§Ù„Ø­Ø¬', 'Ø§Ø³Ù… Ù…ÙƒØªØ¨ Ø´Ø¤ÙˆÙ† Ø§Ù„Ø­Ø¬Ø§Ø¬ Ø¹Ø±Ø¨ÙŠ']
english_cols = ['Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ© Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ', 'Ø§Ø³Ù… Ù…ÙƒØªØ¨ Ø´Ø¤ÙˆÙ† Ø§Ù„Ø­Ø¬Ø§Ø¬ Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ', 'Hajj Company']

df_final['Hajj Company (Arabic)'] = None
df_final['Hajj Company (English)'] = None

for col in arabic_cols:
    if col in df_final.columns:
        df_final['Hajj Company (Arabic)'] = df_final['Hajj Company (Arabic)'].fillna(df_final[col])

for col in english_cols:
    if col in df_final.columns:
        df_final['Hajj Company (English)'] = df_final['Hajj Company (English)'].fillna(df_final[col])

# 4ï¸âƒ£ Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆØ§Ù„Ø¯ÙˆÙ„Ø©
city_columns = ['Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©', 'City']
country_columns = ['Ø§Ù„Ø¯ÙˆÙ„Ø©', 'Country']

df_final['City'] = None
for col in city_columns:
    if col in df_final.columns:
        df_final['City'] = df_final['City'].fillna(df_final[col])

df_final['Country'] = None
for col in country_columns:
    if col in df_final.columns:
        df_final['Country'] = df_final['Country'].fillna(df_final[col])

# 5ï¸âƒ£ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
cols_to_remove = ['Ù†ÙˆØ¹ Ø§Ù„Ø¨Ø§Ù‚Ø©', 'Ø§Ù„Ø³Ù†Ø© Ø¨Ø§Ù„Ù‡Ø¬Ø±ÙŠ', 'Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø¬Ø§Ø¬', 'Ø§Ù„Ù†ÙˆØ¹']
df_final.drop(columns=[c for c in cols_to_remove if c in df_final.columns], inplace=True)

# 6ï¸âƒ£ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ
if 'Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠ' in df_final.columns:
    df_final.rename(columns={'Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠ': 'email'}, inplace=True)
elif 'Email' in df_final.columns:
    df_final.rename(columns={'Email': 'email'}, inplace=True)
df_final['email'] = df_final['email'].fillna('ØºÙŠØ± Ù…ØªÙˆÙØ±')

# 7ï¸âƒ£ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ù…Ø¯Ù†
saudi_cities = ['Ù…ÙƒØ©','Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©','Ø§Ù„Ø±ÙŠØ§Ø¶','Ø¬Ø¯Ø©','Ø§Ù„Ø¯Ù…Ø§Ù…','Ø§Ù„Ø·Ø§Ø¦Ù','Ø§Ù„Ù‚Ø·ÙŠÙ','ÙŠÙ†Ø¨Ø¹','ØªØ¨ÙˆÙƒ',
                'Ø£Ø¨Ù‡Ø§','Ù†Ø¬Ø±Ø§Ù†','Ø§Ù„Ù‚ØµÙŠÙ…','Ø§Ù„Ø®Ø¨Ø±','Ø¹Ø±Ø¹Ø±','Ø­Ø§Ø¦Ù„','Ø§Ù„Ø¨Ø§Ø­Ø©','Ø¬Ø§Ø²Ø§Ù†']

df_final['Country'] = df_final.apply(
    lambda row: 'Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©'
    if pd.notna(row['City']) and any(city in str(row['City']) for city in saudi_cities)
    else row['Country'], axis=1
)

# 8ï¸âƒ£ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
unique_cols = ['Hajj Company (Arabic)', 'Hajj Company (English)', 'City', 'Country', 'email']
df_final = df_final.sort_values(by='email').drop_duplicates(subset=unique_cols).reset_index(drop=True)

# 9ï¸âƒ£ ØªØ¹Ø¨Ø¦Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¯Ø§Ø®Ù„ Ù†ÙØ³ Ø§Ù„Ø´Ø±ÙƒØ©
for col in ['City', 'Country', 'email']:
    df_final[col] = df_final.groupby('Hajj Company (Arabic)')[col].transform(lambda x: x.ffill().bfill())

# 10ï¸âƒ£ Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
df_final[['City','Country','email']] = df_final[['City','Country','email']].fillna(['ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ','ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ','ØºÙŠØ± Ù…ØªÙˆÙØ±'])
df_final['is_autorized'] = 'Yes'

# 11ï¸âƒ£ Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©
df_final = df_final[['Hajj Company (Arabic)', 'Hajj Company (English)', 'City', 'Country', 'email', 'is_autorized']]
df_final.to_excel('Hajj_Info_Final.xlsx', index=False)
print("âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„Ù: Hajj_Info_Final.xlsx")


# ============================================================
# ğŸ”¹ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªØ±Ø¬Ù…Ø© Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ØµÙØ­Ø§Øª (Ø§Ù„ÙˆÙƒØ§Ù„Ø§Øª)
# ============================================================
from langdetect import detect
from googletrans import Translator

df_pages = pd.read_excel('unique_page_names.xlsx')

# ÙƒØ´Ù Ø§Ù„Ù„ØºØ©
def detect_lang(text):
    try:
        return detect(str(text))
    except:
        return None

df_pages['detected_language'] = df_pages.iloc[:, 0].apply(detect_lang)
df_pages.rename(columns={df_pages.columns[0]: 'Name'}, inplace=True)

# ÙØµÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
df_pages['Name_en'] = df_pages.apply(lambda r: r['Name'] if r['detected_language'] != 'ar' else None, axis=1)
df_pages['Name_ar'] = df_pages.apply(lambda r: r['Name'] if r['detected_language'] == 'ar' else None, axis=1)

# ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…ÙÙ‚ÙˆØ¯
translator = Translator()

def translate_text(text, src, dest):
    try:
        return translator.translate(str(text), src=src, dest=dest).text
    except:
        return None

df_pages.loc[df_pages['Name_ar'].isna() & df_pages['Name_en'].notna(), 'Name_ar'] = \
    df_pages.loc[df_pages['Name_ar'].isna() & df_pages['Name_en'].notna(), 'Name_en'].apply(lambda x: translate_text(x, 'en', 'ar'))

df_pages.loc[df_pages['Name_en'].isna() & df_pages['Name_ar'].notna(), 'Name_en'] = \
    df_pages.loc[df_pages['Name_en'].isna() & df_pages['Name_ar'].notna(), 'Name_ar'].apply(lambda x: translate_text(x, 'ar', 'en'))

df_pages.drop(columns=['Name', 'detected_language'], inplace=True)
df_pages.rename(columns={'Name_en':'Hajj Company (English)', 'Name_ar':'Hajj Company (Arabic)'}, inplace=True)
df_pages.to_excel('translated_hajj_agencies.xlsx', index=False)
print("âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„Ù: translated_hajj_agencies.xlsx")

# ============================================================
# ğŸ”¹ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø´Ø±ÙƒØ§Øª (Fuzzy Matching)
# ============================================================
# !pip install rapidfuzz textdistance sentence-transformers openpyxl -q
from rapidfuzz import fuzz, process
from difflib import SequenceMatcher
import textdistance
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

print("\nğŸ“‚ Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª...")
file1 = pd.read_excel('translated_hajj_agencies.xlsx')
file2 = pd.read_excel('Hajj_Info_Final.xlsx')

# ØªÙ†Ø¸ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
file2_clean = file2.dropna(subset=['Hajj Company (English)']).copy()
file2_clean = file2_clean[file2_clean['Hajj Company (English)'].str.strip() != '']
file2_list = file2_clean['Hajj Company (English)'].str.lower().tolist()

# ------------------------------------------------------------
# ğŸ§© Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø©
# ------------------------------------------------------------
def method_1_token_set(query, reference_list, threshold=70):
    result = process.extractOne(query.lower().strip(), reference_list, scorer=fuzz.token_set_ratio)
    if result and result[1] >= threshold:
        return result[0], result[1]
    return None, 0

def method_2_ratio(query, reference_list, threshold=70):
    result = process.extractOne(query.lower().strip(), reference_list, scorer=fuzz.ratio)
    if result and result[1] >= threshold:
        return result[0], result[1]
    return None, 0

def method_3_difflib(query, reference_list, threshold=0.7):
    best, score = None, 0
    for ref in reference_list:
        s = SequenceMatcher(None, query.lower().strip(), ref).ratio()
        if s > score:
            best, score = ref, s
    return (best, score*100) if score >= threshold else (None, 0)

def method_4_levenshtein(query, reference_list, threshold=0.7):
    best, score = None, 0
    for ref in reference_list:
        distance = textdistance.levenshtein.distance(query.lower(), ref)
        max_len = max(len(query), len(ref))
        sim = (1 - distance/max_len) if max_len > 0 else 0
        if sim > score:
            best, score = ref, sim
    return (best, score*100) if score >= threshold else (None, 0)

# ------------------------------------------------------------
# ğŸ¤– Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù€ Embeddings
# ------------------------------------------------------------
print("ğŸ¤– ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embeddings...")
model = SentenceTransformer('all-MiniLM-L6-v2')
file2_embeddings = model.encode(file2_list, show_progress_bar=True)

def method_5_embeddings(query, ref_embeddings, ref_list, threshold=0.7):
    query_emb = model.encode([query.lower().strip()])
    sims = cosine_similarity(query_emb, ref_embeddings)[0]
    idx = np.argmax(sims)
    score = sims[idx]
    return (ref_list[idx], score*100) if score >= threshold else (None, 0)

# ============================================================
# ğŸ”¹ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ÙˆØ­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
# ============================================================
comparison_results = []

print("\nğŸ§ª Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø¹Ù„Ù‰ Ø£ÙˆÙ„ 10 Ø´Ø±ÙƒØ§Øª...\n")
for idx, row in file1.head(10).iterrows():
    company = str(row['Hajj Company (English)'])
    if not company.strip():
        continue

    print(f"ğŸ” {company}")
    result = {'Original': company}

    for i, (func, name) in enumerate([
        (method_1_token_set, "RapidFuzz token_set"),
        (method_2_ratio, "RapidFuzz ratio"),
        (method_3_difflib, "difflib"),
        (method_4_levenshtein, "Levenshtein"),
        (method_5_embeddings, "Embeddings")
    ], 1):
        start = time.time()
        if name == "Embeddings":
            match, score = func(company, file2_embeddings, file2_list)
        else:
            match, score = func(company, file2_list)
        t = (time.time() - start) * 1000
        print(f" {i}) {name:<20} | Match: {match[:40] if match else '-':40} | Score: {score:.1f}% | {t:.2f}ms")
        result[f'{name} Match'] = match
        result[f'{name} Score'] = round(score, 1)
        result[f'{name} Time (ms)'] = round(t, 2)

    comparison_results.append(result)

comparison_df = pd.DataFrame(comparison_results)

# ------------------------------------------------------------
# ğŸ§¾ Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡
# ------------------------------------------------------------
summary = pd.DataFrame({
    'Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©': [
        'RapidFuzz token_set', 'RapidFuzz ratio', 'difflib', 'Levenshtein', 'Embeddings'
    ],
    'Ù…ØªÙˆØ³Ø· Ø§Ù„Ù†Ø³Ø¨Ø© %': [
        comparison_df[c].mean() for c in comparison_df.columns if 'Score' in c
    ],
    'Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆÙ‚Øª (ms)': [
        comparison_df[c].mean() for c in comparison_df.columns if 'Time' in c
    ],
    'Ø¹Ø¯Ø¯ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª': [
        (comparison_df[c] > 0).sum() for c in comparison_df.columns if 'Score' in c
    ]
})

# ------------------------------------------------------------
# ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
# ------------------------------------------------------------
with pd.ExcelWriter('fuzzy_matching_comparison.xlsx', engine='openpyxl') as writer:
    comparison_df.to_excel(writer, sheet_name='ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©', index=False)
    summary.to_excel(writer, sheet_name='Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡', index=False)

print("\nâœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©: fuzzy_matching_comparison.xlsx")
print("\nğŸ¯ Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©:", summary.loc[summary['Ù…ØªÙˆØ³Ø· Ø§Ù„Ù†Ø³Ø¨Ø© %'].idxmax(), 'Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©'])
print("âš¡ Ø§Ù„Ø£Ø³Ø±Ø¹:", summary.loc[summary['Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆÙ‚Øª (ms)'].idxmin(), 'Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©'])
