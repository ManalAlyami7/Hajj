# -----------------------------
# Voice Bot Interface
# -----------------------------
if "show_voice_interface" not in st.session_state:
    st.session_state.show_voice_interface = False

# Floating Microphone Button
st.markdown("""
<style>
    .floating-mic {
        position: fixed;
        bottom: 100px;
        right: 30px;
        z-index: 9999;
    }
    
    .mic-button {
        width: 60px;
        height: 60px;
        border-radius: 50%;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border: none;
        box-shadow: 0 4px 20px rgba(102, 126, 234, 0.5);
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.3s ease;
        animation: pulse 2s infinite;
    }
    
    .mic-button:hover {
        transform: scale(1.1);
        box-shadow: 0 6px 30px rgba(102, 126, 234, 0.7);
    }
    
    @keyframes pulse {
        0%, 100% {
            box-shadow: 0 4px 20px rgba(102, 126, 234, 0.5);
        }
        50% {
            box-shadow: 0 4px 30px rgba(102, 126, 234, 0.8);
        }
    }
    
    .voice-interface {
        background: rgba(255, 255, 255, 0.98);
        backdrop-filter: blur(10px);
        border-radius: 20px;
        padding: 3rem;
        margin: 2rem auto;
        max-width: 800px;
        box-shadow: 0 10px 40px rgba(0, 0, 0, 0.1);
        text-align: center;
    }
    
    .voice-visualizer {
        width: 200px;
        height: 200px;
        border-radius: 50%;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        margin: 2rem auto;
        display: flex;
        align-items: center;
        justify-content: center;
        position: relative;
        animation: voicePulse 1.5s ease-in-out infinite;
    }
    
    @keyframes voicePulse {
        0%, 100% {
            transform: scale(1);
            box-shadow: 0 0 0 0 rgba(102, 126, 234, 0.7);
        }
        50% {
            transform: scale(1.05);
            box-shadow: 0 0 0 20px rgba(102, 126, 234, 0);
        }
    }
    
    .voice-text {
        font-size: 1.5rem;
        color: #667eea;
        margin-top: 1.5rem;
        font-weight: 600;
    }
    
    .voice-status {
        font-size: 1.1rem;
        color: #666;
        margin-top: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# Microphone button in fixed position
mic_col1, mic_col2, mic_col3 = st.columns([6, 1, 1])
with mic_col3:
    if st.button("ğŸ¤", key="mic_button", help="Voice Bot"):
        st.session_state.show_voice_interface = not st.session_state.show_voice_interface
        st.rerun()

# Voice Interface
if st.session_state.show_voice_interface:
    st.markdown("<div class='voice-interface'>", unsafe_allow_html=True)
    
    # Back button
    col1, col2, col3 = st.columns([1, 6, 1])
    with col1:
        if st.button("â† Back", key="back_button"):
            st.session_state.show_voice_interface = False
            st.rerun()
    
    st.markdown(f"""
    <h2 style='color: #667eea; margin-bottom: 1rem;'>
        ğŸ¤ {'Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØµÙˆØª Ù„Ù„Ø­Ø¬' if st.session_state.new_language == 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' else 'Voice Assistant'}
    </h2>
    """, unsafe_allow_html=True)
    
    # Voice visualizer
    st.markdown("""
    <div class='voice-visualizer'>
        <div style='font-size: 4rem;'>ğŸ¤</div>
    </div>
    """, unsafe_allow_html=True)
    
    # Instructions
    instructions = """
    <div class='voice-text'>
        {} ğŸ§
    </div>
    <div class='voice-status'>
        {}
    </div>
    """.format(
        "Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø± Ø£Ø¯Ù†Ø§Ù‡ ÙˆØªØ­Ø¯Ø«" if st.session_state.new_language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "Click the button below and speak",
        "Ø³ÙŠØªÙ… ØªØ­ÙˆÙŠÙ„ ØµÙˆØªÙƒ Ø¥Ù„Ù‰ Ù†Øµ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹" if st.session_state.new_language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "Your voice will be converted to text automatically"
    )
    st.markdown(instructions, unsafe_allow_html=True)
    
    # Audio recorder
    st.markdown("<br>", unsafe_allow_html=True)
    
    # File uploader for audio (simulating voice recording)
    audio_file = st.file_uploader(
        "ğŸ™ï¸ " + ("Ø³Ø¬Ù„ ØµÙˆØªÙƒ Ø£Ùˆ Ø§Ø±ÙØ¹ Ù…Ù„Ù ØµÙˆØªÙŠ" if st.session_state.new_language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "Record or upload audio"),
        type=["wav", "mp3", "m4a", "ogg"],
        key="audio_upload"
    )
    
    if audio_file is not None:
        st.audio(audio_file)
        
        with st.spinner("ğŸ”„ " + ("Ø¬Ø§Ø±Ù ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ..." if st.session_state.new_language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "Converting speech to text...")):
            try:
                # Transcribe audio using OpenAI Whisper
                transcription = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="ar" if st.session_state.new_language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "en"
                )
                
                transcribed_text = transcription.text
                
                st.success("âœ… " + ("ØªÙ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!" if st.session_state.new_language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "Transcription successful!"))
                st.info(f"**{'Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­ÙˆÙ„' if st.session_state.new_language == 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' else 'Transcribed Text'}:** {transcribed_text}")
                
                # Process the transcribed text
                if st.button("ğŸ” " + ("Ø¨Ø­Ø«" if st.session_state.new_language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "Search"), type="primary"):
                    st.session_state.selected_question = transcribed_text
                    st.session_state.show_voice_interface = False
                    st.rerun()
                    
            except Exception as e:
                st.error(f"âŒ {'Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­ÙˆÙŠÙ„' if st.session_state.new_language == 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' else 'Transcription error'}: {str(e)}")
    
    # Alternative: Text-to-Speech for responses
    st.markdown("<br><hr><br>", unsafe_allow_html=True)
    st.markdown(f"""
    <h3 style='color: #667eea;'>
        ğŸ”Š {'Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„Ø±Ø¯ÙˆØ¯' if st.session_state.new_language == 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' else 'Listen to Responses'}
    </h3>
    """, unsafe_allow_html=True)
    
    if st.session_state.chat_memory and len(st.session_state.chat_memory) > 1:
        last_response = st.session_state.chat_memory[-1]
        if last_response.get("role") == "assistant":
            response_text = last_response.get("content", "")
            
            if st.button("ğŸ”Š " + ("Ø§Ø³ØªÙ…Ø¹ Ù„Ù„Ø±Ø¯ Ø§Ù„Ø£Ø®ÙŠØ±" if st.session_state.new_language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "Listen to Last Response")):
                with st.spinner("ğŸµ " + ("Ø¬Ø§Ø±Ù ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª..." if st.session_state.new_language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "Generating audio...")):
                    try:
                        # Generate speech using OpenAI TTS
                        speech_response = client.audio.speech.create(
                            model="tts-1",
                            voice="alloy",
                            input=response_text[:500]  # Limit to 500 chars for demo
                        )
                        
                        # Save to temporary file
                        import tempfile
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp_file:
                            tmp_file.write(speech_response.content)
                            tmp_file_path = tmp_file.name
                        
                        # Play audio
                        with open(tmp_file_path, "rb") as audio:
                            st.audio(audio.read(), format="audio/mp3")
                            
                    except Exception as e:
                        st.error(f"âŒ {'Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª' if st.session_state.new_language == 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' else 'Audio generation error'}: {str(e)}")
    
    st.markdown("</div>", unsafe_allow_html=True)

# -----------------------------
# EOF
# -----------------------------
