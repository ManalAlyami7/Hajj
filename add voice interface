# -----------------------------
# Voice Bot Interface
# -----------------------------
if "show_voice_interface" not in st.session_state:
    st.session_state.show_voice_interface = False

# Floating Microphone Button
st.markdown("""
<style>
    .floating-mic {
        position: fixed;
        bottom: 100px;
        right: 30px;
        z-index: 9999;
    }
    
    .mic-button {
        width: 60px;
        height: 60px;
        border-radius: 50%;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border: none;
        box-shadow: 0 4px 20px rgba(102, 126, 234, 0.5);
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.3s ease;
        animation: pulse 2s infinite;
    }
    
    .mic-button:hover {
        transform: scale(1.1);
        box-shadow: 0 6px 30px rgba(102, 126, 234, 0.7);
    }
    
    @keyframes pulse {
        0%, 100% {
            box-shadow: 0 4px 20px rgba(102, 126, 234, 0.5);
        }
        50% {
            box-shadow: 0 4px 30px rgba(102, 126, 234, 0.8);
        }
    }
    
    .voice-interface {
        background: rgba(255, 255, 255, 0.98);
        backdrop-filter: blur(10px);
        border-radius: 20px;
        padding: 3rem;
        margin: 2rem auto;
        max-width: 800px;
        box-shadow: 0 10px 40px rgba(0, 0, 0, 0.1);
        text-align: center;
    }
    
    .voice-visualizer {
        width: 200px;
        height: 200px;
        border-radius: 50%;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        margin: 2rem auto;
        display: flex;
        align-items: center;
        justify-content: center;
        position: relative;
        animation: voicePulse 1.5s ease-in-out infinite;
    }
    
    @keyframes voicePulse {
        0%, 100% {
            transform: scale(1);
            box-shadow: 0 0 0 0 rgba(102, 126, 234, 0.7);
        }
        50% {
            transform: scale(1.05);
            box-shadow: 0 0 0 20px rgba(102, 126, 234, 0);
        }
    }
    
    .voice-text {
        font-size: 1.5rem;
        color: #667eea;
        margin-top: 1.5rem;
        font-weight: 600;
    }
    
    .voice-status {
        font-size: 1.1rem;
        color: #666;
        margin-top: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# Microphone button in fixed position
mic_col1, mic_col2, mic_col3 = st.columns([6, 1, 1])
with mic_col3:
    if st.button("🎤", key="mic_button", help="Voice Bot"):
        st.session_state.show_voice_interface = not st.session_state.show_voice_interface
        st.rerun()

# Voice Interface
if st.session_state.show_voice_interface:
    st.markdown("<div class='voice-interface'>", unsafe_allow_html=True)
    
    # Back button
    col1, col2, col3 = st.columns([1, 6, 1])
    with col1:
        if st.button("← Back", key="back_button"):
            st.session_state.show_voice_interface = False
            st.rerun()
    
    st.markdown(f"""
    <h2 style='color: #667eea; margin-bottom: 1rem;'>
        🎤 {'مساعد الصوت للحج' if st.session_state.new_language == 'العربية' else 'Voice Assistant'}
    </h2>
    """, unsafe_allow_html=True)
    
    # Voice visualizer
    st.markdown("""
    <div class='voice-visualizer'>
        <div style='font-size: 4rem;'>🎤</div>
    </div>
    """, unsafe_allow_html=True)
    
    # Instructions
    instructions = """
    <div class='voice-text'>
        {} 🎧
    </div>
    <div class='voice-status'>
        {}
    </div>
    """.format(
        "اضغط على الزر أدناه وتحدث" if st.session_state.new_language == "العربية" else "Click the button below and speak",
        "سيتم تحويل صوتك إلى نص تلقائياً" if st.session_state.new_language == "العربية" else "Your voice will be converted to text automatically"
    )
    st.markdown(instructions, unsafe_allow_html=True)
    
    # Audio recorder
    st.markdown("<br>", unsafe_allow_html=True)
    
    # File uploader for audio (simulating voice recording)
    audio_file = st.file_uploader(
        "🎙️ " + ("سجل صوتك أو ارفع ملف صوتي" if st.session_state.new_language == "العربية" else "Record or upload audio"),
        type=["wav", "mp3", "m4a", "ogg"],
        key="audio_upload"
    )
    
    if audio_file is not None:
        st.audio(audio_file)
        
        with st.spinner("🔄 " + ("جارٍ تحويل الصوت إلى نص..." if st.session_state.new_language == "العربية" else "Converting speech to text...")):
            try:
                # Transcribe audio using OpenAI Whisper
                transcription = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="ar" if st.session_state.new_language == "العربية" else "en"
                )
                
                transcribed_text = transcription.text
                
                st.success("✅ " + ("تم التحويل بنجاح!" if st.session_state.new_language == "العربية" else "Transcription successful!"))
                st.info(f"**{'النص المحول' if st.session_state.new_language == 'العربية' else 'Transcribed Text'}:** {transcribed_text}")
                
                # Process the transcribed text
                if st.button("🔍 " + ("بحث" if st.session_state.new_language == "العربية" else "Search"), type="primary"):
                    st.session_state.selected_question = transcribed_text
                    st.session_state.show_voice_interface = False
                    st.rerun()
                    
            except Exception as e:
                st.error(f"❌ {'حدث خطأ في التحويل' if st.session_state.new_language == 'العربية' else 'Transcription error'}: {str(e)}")
    
    # Alternative: Text-to-Speech for responses
    st.markdown("<br><hr><br>", unsafe_allow_html=True)
    st.markdown(f"""
    <h3 style='color: #667eea;'>
        🔊 {'الاستماع للردود' if st.session_state.new_language == 'العربية' else 'Listen to Responses'}
    </h3>
    """, unsafe_allow_html=True)
    
    if st.session_state.chat_memory and len(st.session_state.chat_memory) > 1:
        last_response = st.session_state.chat_memory[-1]
        if last_response.get("role") == "assistant":
            response_text = last_response.get("content", "")
            
            if st.button("🔊 " + ("استمع للرد الأخير" if st.session_state.new_language == "العربية" else "Listen to Last Response")):
                with st.spinner("🎵 " + ("جارٍ توليد الصوت..." if st.session_state.new_language == "العربية" else "Generating audio...")):
                    try:
                        # Generate speech using OpenAI TTS
                        speech_response = client.audio.speech.create(
                            model="tts-1",
                            voice="alloy",
                            input=response_text[:500]  # Limit to 500 chars for demo
                        )
                        
                        # Save to temporary file
                        import tempfile
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp_file:
                            tmp_file.write(speech_response.content)
                            tmp_file_path = tmp_file.name
                        
                        # Play audio
                        with open(tmp_file_path, "rb") as audio:
                            st.audio(audio.read(), format="audio/mp3")
                            
                    except Exception as e:
                        st.error(f"❌ {'حدث خطأ في توليد الصوت' if st.session_state.new_language == 'العربية' else 'Audio generation error'}: {str(e)}")
    
    st.markdown("</div>", unsafe_allow_html=True)

# -----------------------------
# EOF
# -----------------------------
