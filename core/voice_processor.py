"""
Voice Processor Module - OPTIMIZED OPENAI VERSION (Streaming)
Drop-in replacement for your existing voice_processor.py
Uses OpenAI Whisper + GPT + TTS with streaming for 2x speed improvement
"""

import streamlit as st
from openai import AsyncOpenAI, OpenAI
import io
from typing import Dict, Optional, AsyncGenerator
import logging
import difflib
from sqlalchemy import text

import asyncio

from core.voice_models import (
    VoiceIntentClassification,
    VoiceResponse,
    DatabaseVoiceResponse
)
from core.database import DatabaseManager
from core.voice_llm import LLMManager

# Configure logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
from difflib import get_close_matches
from sqlalchemy import text
import openai
from rapidfuzz import process, fuzz


class VoiceQueryProcessor:
    def __init__(self, db, voice_llm):
        self.db = db
        self.voice_llm = voice_llm
        self.client = self.voice_llm._get_client()

    def get_agency_names(self):
        # Get agency names from DB (Arabic + English)
        engine = self.db._create_engine()
        with engine.connect() as conn:
            result = conn.execute(text("SELECT hajj_company_ar, hajj_company_en FROM agencies"))
            return [ar or en for ar, en in result.fetchall() if ar or en]


    def correct_transcript(self, raw_text, agency_names, threshold=80):
        
        # Step 2: Fuzzy match to all agency names locally
        matches = process.extract(
            raw_text,            # text to match
            agency_names,            # list of names
            scorer=fuzz.token_sort_ratio,
            score_cutoff=threshold   # only matches above threshold
        )

        # Return all matched names (official names)
        matched_names = [match[0] for match in matches]
        return matched_names if matched_names else [raw_text]




class VoiceProcessor:
    """
    Optimized Voice Processor with OpenAI streaming.
    Drop-in replacement with 2x performance improvement.
    """
    
    def __init__(self):
        """Initialize Voice Processor with OpenAI client, database, and LLM manager."""
        self.client = self._get_client()
        self.async_client = self._get_async_client()
        self.db = DatabaseManager()
        self.voice_llm = LLMManager()

    # --- Internal Methods -----------------------------------------------------
    @st.cache_resource
    def _get_client(_self):
        """Get cached OpenAI client (sync version for compatibility)."""
        api_key = st.secrets.get("OPENAI_API_KEY") or st.secrets.get("key")
        if not api_key:
            logger.error("OpenAI API key not found in Streamlit secrets.")
            st.error("‚ö†Ô∏è Please add your `OPENAI_API_KEY` to Streamlit secrets.")
            st.stop()
        return OpenAI(api_key=api_key)
    
    @st.cache_resource
    def _get_async_client(_self):
        """Get cached async OpenAI client for streaming."""
        api_key = st.secrets.get("OPENAI_API_KEY") or st.secrets.get("key")
        if not api_key:
            logger.error("OpenAI API key not found in Streamlit secrets.")
            st.error("‚ö†Ô∏è Please add your `OPENAI_API_KEY` to Streamlit secrets.")
            st.stop()
        return AsyncOpenAI(api_key=api_key)



    # --- Audio Transcription (OPTIMIZED) --------------------------------------
    def transcribe_audio(self, audio_bytes: bytes) -> Dict:
        """Transcribe recorded audio into text and detect language (OPTIMIZED)."""
        try:
            audio_file = io.BytesIO(audio_bytes)
            audio_file.name = "audio.wav"
            logger.info("üéôÔ∏è Sending audio for transcription...")

            # Context prompt to guide Whisper transcription
            # This helps with domain-specific terms and reduces errors


            # OPTIMIZATION: Use text format instead of verbose_json (faster)
            # OPTIMIZATION: Set temperature to 0 for faster, deterministic results
            transcript = self.client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                response_format="text",  # FASTER than verbose_json
                temperature=0.0,  # More deterministic = faster
                language=None,  # Auto-detect language
                #prompt=context_prompt  # NEW: Contextual guidance for better accuracy
            )
            
            text = transcript if isinstance(transcript, str) else getattr(transcript, "text", "")
            
            # Auto-detect language from text
            language = "arabic" if any("\u0600" <= ch <= "\u06FF" for ch in text) else "english"
            
            # Fix common transcription errors for "Hajj" (keep as fallback)
            proc = VoiceQueryProcessor(self.db, self.voice_llm)

            agency_names = proc.get_agency_names()


            matched_names = proc.correct_transcript(text, agency_names)

            if len(matched_names) == 1:
                cleaned_text = matched_names[0]
            else:
                # multiple possible matches
                cleaned_text = ", ".join(matched_names)



            result = {
                "text": text.strip(),
                "language": language,
                "confidence": 1.0,
                "duration": 0  # Not available in text format
            }

            logger.info(f"üéôÔ∏è Transcribed: '{text[:60]}...' (lang: {language})")
            return result

        except Exception as e:
            logger.error(f"‚ùå Transcription failed: {e}")
            return {
                "text": "",
                "language": "en",
                "confidence": 0.0,
                "error": str(e)
            }

    # --- Text-to-Speech (OPTIMIZED WITH CHUNKING) -----------------------------
    def text_to_speech(self, text: str, language: str = "en") -> Optional[bytes]:
        """
        Convert text to speech and return MP3 bytes (OPTIMIZED).
        For long text, consider using text_to_speech_chunked() instead.
        """
        if not text or not text.strip():
            logger.warning("‚ö†Ô∏è Empty text provided to TTS.")
            return None

        # Voice mapping based on language
        voice_map = {
            "ar": "onyx",   # Arabic - deep, formal tone
            "arabic": "onyx",
            "en": "alloy",  # English - neutral tone
            "english": "alloy",
            "ur": "alloy",
            "urdu": "alloy",
            "id": "alloy",
            "indonesian": "alloy",
            "tr": "alloy",
            "turkish": "alloy"
        }

        voice = voice_map.get(language, "alloy")

        try:
            # OPTIMIZATION: Use tts-1 instead of tts-1-hd (2x faster)
            # OPTIMIZATION: Increase speed to 1.1 (10% faster speech)
            response = self.client.audio.speech.create(
                model="tts-1",  # FASTER than tts-1-hd
                voice=voice,
                input=text.strip()[:4096],  # Limit to 4096 chars for speed
                speed=1.1,  # 10% faster speech
                response_format="mp3"
            )

            audio_bytes = response.read()
            logger.info(f"üîä TTS generated for: '{text[:60]}...' (lang: {language})")
            return audio_bytes

        except Exception as e:
            logger.error(f"‚ùå TTS generation failed: {e}")
            return None
    
    # --- NEW: STREAMING TTS FOR LONG TEXT ------------------------------------
    def text_to_speech_chunked(self, text: str, language: str = "en") -> list[bytes]:
        """
        Convert long text to speech in chunks for faster perceived latency.
        Returns list of audio chunks that can be played sequentially.
        """
        if not text or not text.strip():
            return []
        
        # Split text into sentences
        import re
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        
        audio_chunks = []
        current_chunk = ""
        
        voice_map = {
            "ar": "onyx", "arabic": "onyx",
            "en": "alloy", "english": "alloy",
            "ur": "alloy", "urdu": "alloy",
            "id": "alloy", "indonesian": "alloy",
            "tr": "alloy", "turkish": "alloy"
        }
        voice = voice_map.get(language, "alloy")
        
        try:
            for sentence in sentences:
                current_chunk += sentence + " "
                
                # Generate audio when chunk is big enough (40-150 chars)
                if len(current_chunk) >= 40:
                    response = self.client.audio.speech.create(
                        model="tts-1",
                        voice=voice,
                        input=current_chunk.strip(),
                        speed=1.1
                    )
                    audio_chunks.append(response.read())
                    current_chunk = ""
            
            # Generate remaining text
            if current_chunk.strip():
                response = self.client.audio.speech.create(
                    model="tts-1",
                    voice=voice,
                    input=current_chunk.strip(),
                    speed=1.1
                )
                audio_chunks.append(response.read())
            
            return audio_chunks
            
        except Exception as e:
            logger.error(f"‚ùå Chunked TTS failed: {e}")
            return []
    
    # --- NEW: ASYNC STREAMING FOR MAXIMUM SPEED -------------------------------
    async def text_to_speech_stream_async(
        self, 
        text_generator: AsyncGenerator[str, None],
        language: str = "en"
    ) -> AsyncGenerator[bytes, None]:
        """
        Stream TTS generation as text is being generated by LLM.
        Use this with streaming LLM responses for minimum latency.
        
        Example:
            async for audio_chunk in processor.text_to_speech_stream_async(llm_stream):
                # Play audio_chunk immediately
        """
        voice_map = {
            "ar": "onyx", "arabic": "onyx",
            "en": "alloy", "english": "alloy",
            "ur": "alloy", "urdu": "alloy",
            "id": "alloy", "indonesian": "alloy",
            "tr": "alloy", "turkish": "alloy"
        }
        voice = voice_map.get(language, "alloy")
        
        accumulated_text = ""
        
        try:
            async for text_chunk in text_generator:
                accumulated_text += text_chunk
                
                # Generate audio at sentence boundaries
                if len(accumulated_text) >= 40 and text_chunk[-1] in '.!?\n':
                    response = await self.async_client.audio.speech.create(
                        model="tts-1",
                        voice=voice,
                        input=accumulated_text.strip(),
                        speed=1.1
                    )
                    
                    audio_bytes = await response.aread()
                    yield audio_bytes
                    accumulated_text = ""
            
            # Generate remaining text
            if accumulated_text.strip():
                response = await self.async_client.audio.speech.create(
                    model="tts-1",
                    voice=voice,
                    input=accumulated_text.strip(),
                    speed=1.1
                )
                audio_bytes = await response.aread()
                yield audio_bytes
                
        except Exception as e:
            logger.error(f"‚ùå Async streaming TTS failed: {e}")